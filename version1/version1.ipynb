{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0266f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5f877b9",
   "metadata": {},
   "source": [
    "# Programming Exercise using PySpark\n",
    "\n",
    "## Background:\n",
    "A very small company called **KommatiPara** that deals with bitcoin trading has two separate datasets dealing with clients that they want to collate to starting interfacing more with their clients. One dataset contains information about the clients and the other one contains information about their financial details.\n",
    "\n",
    "The company now needs a dataset containing the emails of the clients from the United Kingdom and the Netherlands and some of their financial details to starting reaching out to them for a new marketing push.\n",
    "\n",
    "Since all the data in the datasets is fake and this is just an exercise, one can forego the issue of having the data stored along with the code in a code repository.\n",
    "\n",
    "\n",
    "## Things to be aware:\n",
    "\n",
    "- Use Python **3.8**\n",
    "- Avoid using notebooks, like **Jupyter** for instance. While these are good for interactive work and/or prototyping in this case they shouldn't be used.\n",
    "- There's no need to use classes, because the assignment is quite small and not very complex in what it does classes are unnecessary.\n",
    "- Only use clients from the **United Kingdom** or the **Netherlands**.\n",
    "- Remove personal identifiable information from the first dataset, **excluding emails**.\n",
    "- Remove credit card number from the second dataset.\n",
    "- Data should be joined using the **id** field.\n",
    "- Rename the columns for the easier readability to the business users:\n",
    "\n",
    "|Old name|New name|\n",
    "|--|--|\n",
    "|id|client_identifier|\n",
    "|btc_a|bitcoin_address|\n",
    "|cc_t|credit_card_type|\n",
    "\n",
    "- The project should be stored in GitHub and you should only commit relevant files to the repo.\n",
    "- Save the output in a **client_data** directory in the root directory of the project.\n",
    "- Add a **README** file explaining on a high level what the application does.\n",
    "- Application should receive three arguments, the paths to each of the dataset files and also the countries to filter as the client wants to reuse the code for other countries.\n",
    "- Use **logging**.\n",
    "- Create generic functions for filtering data and renaming.\n",
    "Recommendation: Use the following package for Spark tests - https://github.com/MrPowers/chispa\n",
    "- If possible, have different branches for different tasks that once completed are merged to the main branch. Follow the GitHub flow - https://guides.github.com/introduction/flow/.\n",
    "- **Bonus** - If possible it should have an automated build pipeline using GitHub Actions - https://docs.github.com/en/actions - or Travis - https://www.travis-ci.com/ for instance.\n",
    "- **Bonus** - If possible log to a file with a rotating policy.\n",
    "- **Bonus** - Code should be able to be packaged into a source distribution file.\n",
    "- **Bonus** - Requirements file should exist.\n",
    "- **Bonus** - Document the code with docstrings as much as possible using the reStructuredText (reST) format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42976ddb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 67\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# dbutils.fs.cp(location + \"tmp/\" + name, location + filename + \".csv\")\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# dbutils.fs.rm(location + \"tmp\", recurse = True)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mWriteCsvToLocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#Create generic functions for filtering data and renaming.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 55\u001b[0m, in \u001b[0;36mWriteCsvToLocation\u001b[0;34m(dataframe, location, filename)\u001b[0m\n\u001b[1;32m     52\u001b[0m         name \u001b[38;5;241m=\u001b[39m fileName\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# shutil.copy(os.path.join(location + \"/tmp/\", name), os.path.join(location, filename))            \u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileNameOs \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(location):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"JoinDatasets\").getOrCreate()\n",
    "\n",
    "df1 = spark.read.option('header', True).csv('dataset_one.csv')\n",
    "df2 = spark.read.option('header', True).csv('dataset_two.csv')\n",
    "\n",
    "# Perform join based on the \"id\" column\n",
    "df_joined = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "# drop ccn_n column \n",
    "df_joined=df_joined.drop(\"cc_n\")\\\n",
    "                   .drop(\"first_name\")\\\n",
    "                   .drop(\"last_name\")\n",
    "# Filter with UK and NL\n",
    "filtered_df=df_joined.filter((col(\"country\") == \"United Kingdom\") | (col(\"country\") == \"Netherlands\"))\n",
    "\n",
    "filtered_df = filtered_df.withColumnRenamed(\"id\",\"client_identifier\")\\\n",
    "                         .withColumnRenamed(\"btc_a\",\"bitcoin_address\")\\\n",
    "                         .withColumnRenamed(\"cc_t\", \"credit_card_type\")\n",
    "\n",
    "# Show the result \n",
    "# filtered_df.show()\n",
    "\n",
    "output_folder = \"client_data\"\n",
    "\n",
    "# Create the client_data folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# output_path = \"client_data/dataset_three.csv\"\n",
    "# filtered_df.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "\n",
    "# output_path2 = \"client_data/dataset_three11.csv\"\n",
    "# filtered_df.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").format(\"csv\").save(output_path2)\n",
    "\n",
    "location = \"client_data\"\n",
    "filename = \"dataset_three22.csv\"\n",
    "\n",
    "def WriteCsvToLocation(dataframe, location, filename):\n",
    "\n",
    "    filePathDestTemp = location + \"/tmp\"\n",
    "\n",
    "    dataframe.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").format(\"csv\").save(filePathDestTemp)\n",
    "\n",
    "    \n",
    "    name = ''\n",
    "    for fileName in os.listdir(location):\n",
    "        if fileName.endswith('.csv'):\n",
    "            name = fileName\n",
    "            break\n",
    "    \n",
    "    name.show()\n",
    "\n",
    "    # shutil.copy(os.path.join(location + \"/tmp/\", name), os.path.join(location, filename))            \n",
    "    \n",
    "    for fileNameOs in os.listdir(location):\n",
    "        if fileNameOs != filename:\n",
    "            break\n",
    "\n",
    "    # dbutils.fs.cp(location + \"tmp/\" + name, location + filename + \".csv\")\n",
    "    # dbutils.fs.rm(location + \"tmp\", recurse = True)\n",
    "\n",
    "\n",
    "WriteCsvToLocation(filtered_df,location,filename)\n",
    "\n",
    "\n",
    "#Create generic functions for filtering data and renaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb900554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for reading paruqet files\n",
    "# TODO: play around and read another parquet\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Read paruqet files\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv('dataset_one.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9592278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/15 12:39:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "| country_code|cnt|\n",
      "+-------------+---+\n",
      "|           NL| 62|\n",
      "|           UK| 38|\n",
      "|other country|900|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, when, count\n",
    "\n",
    "# Spark is a little special\n",
    "# it needs the session to be up before we can start with data processing\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Don't worry about the session for now\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "result = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .csv('dataset_one.csv') \\\n",
    "    .filter(col('id') > 0) \\\n",
    "    .withColumn('country_code', \n",
    "        when(col('country').startswith('United Kingdom'), lit('UK')) \\\n",
    "            .otherwise(\n",
    "                when(col('country').startswith('Netherlands'), lit('NL')) \\\n",
    "                    .otherwise(lit('other country'))\n",
    "    )).alias('country_code') \\\n",
    "    .groupBy('country_code') \\\n",
    "    .agg(count('*').alias('cnt'))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6f8ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/09/15 12:53:43 WARN Utils: Your hostname, LCE64920 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/15 12:53:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/15 12:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/15 12:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+--------------+--------------------+--------------------+------------------+\n",
      "| id|first_name| last_name|               email|       country|               btc_a|                cc_t|              cc_n|\n",
      "+---+----------+----------+--------------------+--------------+--------------------+--------------------+------------------+\n",
      "|  1|    Feliza|    Eusden|  feusden0@ameblo.jp|        France|1wjtPamAZeGhRnZfh...|       visa-electron|  4175006996999270|\n",
      "|  2| Priscilla|   Le Pine|plepine1@biglobe....|        France|1Js9BA1rV31hJFmN2...|                 jcb|  3587679584356527|\n",
      "|  3|    Jaimie|    Sandes|jsandes2@reuters.com|        France|1CoG9ciLQVQCnia5o...| diners-club-enroute|   201876885481838|\n",
      "|  4|      Nari|   Dolphin|ndolphin3@cbsloca...|        France|1GNvinVKGzPBVNZSc...|              switch|564182038040530730|\n",
      "|  5|     Garik|     Farre|gfarre4@economist...|        France|1DHTzZ7ypu3EzWtLB...|                 jcb|  3555559025151828|\n",
      "|  6|   Kordula|   Broodes| kbroodes5@amazon.de|        France|1LWktvit3XBCJNrsj...|                 jcb|  3580083825272493|\n",
      "|  7|     Rakel|   Ingliby|    ringliby6@ft.com| United States|1J71SRGqUjhqPuHaZ...|              switch|491193585665108260|\n",
      "|  8|      Derk| Mattielli|dmattielli7@slide...| United States|1Q5FAwgXbhRxP1uYp...|          mastercard|  5100174550682620|\n",
      "|  9|    Karrah|   Boshard|   kboshard8@ihg.com|        France|1QKy8RoeWR48nrwkn...|diners-club-carte...|    30343863921001|\n",
      "| 10| Friedrich|  Kreutzer|fkreutzer9@busine...|        France|1NRDQBCtuDqm8Qomr...|diners-club-carte...|    30559612937267|\n",
      "| 11|      Conn|   Claiden| cclaidena@vimeo.com|        France|1HcqQ5Ys77sJm3ZJv...|                visa|     4937793997478|\n",
      "| 12|     Karel|   Crippin| kcrippinb@google.pl|        France|1EncEr6Vd5ywk96un...|                 jcb|  3569513122126013|\n",
      "| 13| Millisent|     Joint| mjointc@state.tx.us|        France|14bMXV3h1S6KxGHde...|                 jcb|  3537645802098952|\n",
      "| 14|   Valeria|McCloughen| vmccloughend@gov.uk|        France|1Gi1ZJsBDqCztVjtc...|            bankcard|  5602232103395992|\n",
      "| 15|   Monique|  Bernardo|mbernardoe@scient...| United States|1GnNjsnbBTw6w9WHn...|                 jcb|  3558941392668773|\n",
      "| 16|    Callie|    d'Arcy|cdarcyf@people.co...| United States|17y4HG6vY9wDZmeu5...|                 jcb|  3579496825654275|\n",
      "| 17|   Demetri|Bridgwater|dbridgwaterg@youk...| United States|14reD6Z1kUjg8QC5Y...|                 jcb|  3563252716889142|\n",
      "| 18|   Richard|    Drinan|rdrinanh@odnoklas...|United Kingdom|1ErM8yuF3ytzzxLy1...|      china-unionpay| 56022230876188334|\n",
      "| 19|    Benjie|  Stuttman|bstuttmani@cpanel...| United States|1FeH4KecDLZYXEcAu...|          mastercard|  5100176279014886|\n",
      "| 20|  Claresta|Martinetto|cmartinettoj@mapy.cz|        France|14iPptCE59bQXGocz...|                 jcb|  3531510529019283|\n",
      "+---+----------+----------+--------------------+--------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"JoinDatasets\").getOrCreate()\n",
    "\n",
    "# Define the schema for both datasets\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema for the first dataset\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Schema for the second dataset\n",
    "schema2 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"btc_a\", StringType(), True),\n",
    "    StructField(\"cc_t\", StringType(), True),\n",
    "    StructField(\"cc_n\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for the first dataset (first set of data you provided)\n",
    "data1 = [\n",
    "    (1, \"Feliza\", \"Eusden\", \"feusden0@ameblo.jp\", \"France\"),\n",
    "    (2, \"Priscilla\", \"Le Pine\", \"plepine1@biglobe.ne.jp\", \"France\"),\n",
    "    (3, \"Jaimie\", \"Sandes\", \"jsandes2@reuters.com\", \"France\"),\n",
    "    (4, \"Nari\", \"Dolphin\", \"ndolphin3@cbslocal.com\", \"France\"),\n",
    "    (5, \"Garik\", \"Farre\", \"gfarre4@economist.com\", \"France\"),\n",
    "    (6, \"Kordula\", \"Broodes\", \"kbroodes5@amazon.de\", \"France\"),\n",
    "    (7, \"Rakel\", \"Ingliby\", \"ringliby6@ft.com\", \"United States\")\n",
    "]\n",
    "\n",
    "# Sample data for the second dataset (second set of data you provided)\n",
    "data2 = [\n",
    "    (1, \"1wjtPamAZeGhRnZfhBAHHHjNvnHefd2V2\", \"visa-electron\", \"4175006996999270\"),\n",
    "    (2, \"1Js9BA1rV31hJFmN25rh8HWfrrYLXAyw9T\", \"jcb\", \"3587679584356527\"),\n",
    "    (3, \"1CoG9ciLQVQCnia5oXfXPSag4DQ4iYeSpd\", \"diners-club-enroute\", \"201876885481838\"),\n",
    "    (4, \"1GNvinVKGzPBVNZScNA2jKnDSBs4R7Y3rY\", \"switch\", \"564182038040530730\"),\n",
    "    (5, \"1DHTzZ7ypu3EzWtLBFiWoRo9svd1STMyrg\", \"jcb\", \"3555559025151828\"),\n",
    "    (6, \"1LWktvit3XBCJNrsji7rWj2qEa5XAmyJiC\", \"jcb\", \"3580083825272493\"),\n",
    "    (7, \"1J71SRGqUjhqPuHaZaG8wEtKdNRaKUiuzm\", \"switch\", \"491193585665108260\")\n",
    "]\n",
    "\n",
    "# Create DataFrames for both datasets\n",
    "df1 = spark.createDataFrame(data1, schema=schema1)\n",
    "df2 = spark.createDataFrame(data2, schema=schema2)\n",
    "\n",
    "df1 = spark.read.option('header', True).csv('dataset_one.csv')\n",
    "df2 = spark.read.option('header', True).csv('dataset_two.csv')\n",
    "\n",
    "# Perform the join operation based on the \"id\" column\n",
    "df_joined = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "\n",
    "# Show the result of the join\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f55a05",
   "metadata": {},
   "source": [
    "### Reading CSV Files\n",
    "- `spark.read.csv(\"path/to/file.csv\", inferSchema=True, header=True)`\n",
    "\n",
    "Since CSV is just a text file we need to perform some additional actions if we want to know both column names and the data type for each column.\n",
    "\n",
    "In the example above, `inferSchema` determines if Spark should automatically detect the data types of each column based on values, `header` specifies if the first row of data is a header."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a394f",
   "metadata": {},
   "source": [
    "You can also explicitly define the schema when reading data. Than can significantly increase the efficiency of data processing by avoiding the overhead of schema inference.\n",
    "\n",
    "Here is how:\n",
    "- Import necessary types from `pyspark.sql.types`.\n",
    "- You can define the schema using PySpark's `StructType` and `StructField`.\n",
    "- Define the schema and use it in the `read.csv()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f0ca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 16:24:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what happens when we don't use \n",
    "# the header and inferSchema options\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"No Schema Example\").getOrCreate()\n",
    "\n",
    "# Read the CSV file using the defined schema\n",
    "df = spark.read.csv(\"data/taxi_trips_shortened.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d719102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 13:40:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now it's time to use the header option\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Header Example\").getOrCreate()\n",
    "\n",
    "# Read the CSV file using the defined schema\n",
    "df = spark.read.csv(\"data/taxi_trips_shortened.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b24e97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 13:40:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: integer (nullable = true)\n",
      " |-- extra: integer (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: integer (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now it's time to infer the schema\n",
    "# We don't want all columns to be of string type!\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Infer Schema Example\").getOrCreate()\n",
    "\n",
    "# Read the CSV file using the defined schema\n",
    "df = spark.read.csv(\"data/taxi_trips_shortened.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise!\n",
    "# Is infered schema correct?\n",
    "# Is it the same as the one received from the parquet file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58702497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: integer (nullable = true)\n",
      " |-- extra: integer (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 13:41:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Schema inferring can be costly and/or incorrect, \n",
    "# let's specify the schema by hand!\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, \n",
    "StructField, StringType, \\\n",
    "        IntegerType, TimestampType, BooleanType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Schema by hand Example\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "        StructField(\"VendorID\", IntegerType(), True),\n",
    "        StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"lpep_dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"store_and_fwd_flag\", BooleanType(), True),\n",
    "        StructField(\"RatecodeID\", IntegerType(), True),\n",
    "        StructField(\"PULocationID\", IntegerType(), True),\n",
    "        StructField(\"DOLocationID\", IntegerType(), True),\n",
    "        StructField(\"passenger_count\", IntegerType(), True),\n",
    "        StructField(\"trip_distance\", DoubleType(), True),\n",
    "        StructField(\"fare_amount\", IntegerType(), True),\n",
    "        StructField(\"extra\", IntegerType(), True),\n",
    "        StructField(\"mta_tax\", DoubleType(), True),\n",
    "        StructField(\"tip_amount\", IntegerType(), True),\n",
    "        StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "        StructField(\"total_amount\", DoubleType(), True),\n",
    "        StructField(\"payment_type\", IntegerType(), True),\n",
    "        StructField(\"trip_type\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"data/taxi_trips_shortened.csv\", header=True, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448ab50",
   "metadata": {},
   "source": [
    "\n",
    "## Writing Data in PySpark\n",
    "\n",
    "Writing data files involves specifying the format and path. We can also manage how the data is partitioned and the number of output files.\n",
    "\n",
    "To write files to local or external storages we need to call an appropriate method from the `write` object stored in the spark session.\n",
    "\n",
    "### Writing CSV Files\n",
    "- `df.write.csv(\"path/to/output\", mode=\"overwrite\", partitionBy=\"column_name\")`\n",
    "\n",
    "### Writing Parquet Files\n",
    "- `df.write.parquet(\"path/to/output\", mode=\"overwrite\", partitionBy=\"column_name\")`\n",
    "\n",
    "`partitionBy` allows you to specify a column to partition the data upon. Data for each partition will be stored in a separate subdirectory named `<partition_column_name>=<column_value>`. \n",
    "        \n",
    "**In Spark, reading or writing a \"file\" often means reading or writing a directory structure!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a80b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Writing data example\").getOrCreate()\n",
    "\n",
    "# Let's read some data\n",
    "df = spark.read.parquet(\"data/taxi_trips_shortened.parquet\")\n",
    "\n",
    "# Example of writing a DataFrame to CSV\n",
    "df.write.parquet(\n",
    "    \"data/partitioned_data\", mode=\"overwrite\", partitionBy=\"VendorID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485783df",
   "metadata": {},
   "source": [
    "Investigate the `data/partitioned_data` directory and see its structure for yourself! You should see two subdirectories. First only contains data where `VendorID==1` and second only for `VendorID==2`\n",
    "\n",
    "Such split, called `partitioning`, is one of the best methods of improving read performance in big data sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c4d378",
   "metadata": {},
   "source": [
    "## Data Compression in PySpark\n",
    "\n",
    "Compression is an important technique for managing the storage and speed of data processing. PySpark supports various compression codecs for both CSV and Parquet formats.\n",
    "\n",
    "### CSV Compression\n",
    "- CSV files can be compressed with the following codecs: bzip2, gzip, lz4, snappy, and deflate. \n",
    "- Example: `df.write.csv(\"path/to/output.csv\", compression=\"gzip\")`\n",
    "\n",
    "### Parquet Compression\n",
    "- Parquet files inherently support efficient compression and encoding schemes. The default codec is snappy (you don't have specify it), but others like gzip and lzo are also supported.\n",
    "- Example: `df.write.parquet(\"path/to/output.parquet\", compression=\"snappy\")`\n",
    "\n",
    "Using the right compression technique can significantly reduce the disk space used and potentially speed up the read/write operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24386d",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "Now, practice your skills with these exercises.\n",
    "\n",
    "1. Read a CSV file and infer its schema.\n",
    "2. Read a Parquet file and show its first 10 rows.\n",
    "3. Write a DataFrame to CSV format with partition by a specific column.\n",
    "4. Write a DataFrame to Parquet format and control the number of output files.\n",
    "5. Read multiple CSV files from a directory, combine them into one DataFrame, and write to a Parquet file.\n",
    "6. Experiment with different data partition strategies when writing a large DataFrame to multiple output files. Big dataframe can be created from the `data/taxi_trips_full.parquet` file.\n",
    "7. Compare sizes of csv and parquet files that store the same data. With and without compression! Experiment with different codecs! Do all codecs work without any additional dependencies?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
